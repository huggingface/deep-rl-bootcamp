# The Reinforcement Learning Framework [[the-reinforcement-learning-framework]]

## Understanding the RL Process [[the-rl-process]]

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg" alt="The RL process" width="100%">
<figcaption>The RL Process: a loop of state, action, reward and next state.
</figcaption>
<figcaption>Source: <a href="http://incompleteideas.net/book/RLbook2020.pdf">Reinforcement Learning: An Introduction, Richard Sutton and Andrew G. Barto</a></figcaption>
</figure>

Reinforcement Learning is like teaching an agent to play a video game. Imagine you're coaching a player in a platform game:

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg" alt="The RL process" width="100%">

- Our agent starts with an initial **state \\(S_0\\)** from the **Environment**; think of it as the first frame of our game.
- Based on this **state \\(S_0\\)**, the agent makes an **action \\(A_0\\)**; in this case, our agent decides to move to the right.
- This action leads to a **new state \\(S_1\\)**, representing the new frame.
- The environment provides a **reward \\(R_1\\)**; luckily, we're still alive, resulting in a positive reward of +1.

This RL loop generates a sequence of **state, action, reward, and next state.**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg" alt="State, Action, Reward, Next State" width="100%">

The agent's goal is to **maximize** its cumulative reward, which we call the **expected return.**

## The Reward Hypothesis: RL's Central Idea [[reward-hypothesis]]

⇒ Why does the agent aim to maximize expected return?

RL is built on the **reward hypothesis**, which means that all goals can be described as **maximizing expected return** (the expected cumulative reward).

In RL, achieving the **best behavior** means learning to take actions that **maximize the expected cumulative reward.**

## Understanding the Markov Property [[markov-property]]

In academic circles, the RL process is often referred to as a **Markov Decision Process** (MDP).

We'll discuss the Markov Property in depth later, but for now, remember this: the Markov Property implies that our agent **only** needs the **current state** to decide its action, **not the entire history of states and actions** taken previously.

## Observations/States Space [[obs-space]]

Observations/States are the **information our agent receives from the environment**. In a video game, it could be a single frame, like a screenshot. In trading, it might be the value of a stock.

However, it's key to distinguish between *observation* and *state* as explained below:

- *State s*: This is a **complete description of the world** with no hidden information.

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/chess.jpg" alt="Chess">
<figcaption>Game of chess depicting the entire board.
</figcaption>
</figure>

In a fully observed environment, we have access to the entire board, just like in a game of chess.

- *Observation o*: This provides only a **partial description of the state**, a **partially observed environment**.

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg" alt="Mario">
<figcaption>Super Mario Bros screenshot depicting a portion of the environment.
</figcaption>
</figure>

In a **partially observed environment**, such as Super Mario Bros, we can't see the whole level, just the section that is surrounding the character.

<Tip>
To keep it simple, we'll use the term "state" to refer to both state and observation in this course, but we'll distinguish them in practice.
</Tip>

To recap:
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/obs_space_recap.jpg" alt="Obs space recap" width="100%">


## Action Space [[action-space]]

The Action space encompasses **all possible actions** an agent can take in an environment.

Actions can belong to either a *discrete* or *continuous space*:

- *Discrete space*: Here, the number of possible actions is **finite**.

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg" alt="Mario">
<figcaption>Super Mario Bros screenshot depicting the character's carrying out actions.
/figcaption>
</figure>

For example, in Super Mario Bros, there are only four possible actions: left, right, up (jumping), and down (crouching). It is a **finite** set of actions.

- *Continuous space*: This involves an **infinite** number of possible actions.

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/self_driving_car.jpg" alt="Self Driving Car">
<figcaption>Self Driving Car depiction of an agent with infinite possible actions.
</figcaption>
</figure>

For instance, as seen in the above figure, a Self-Driving Car agent can perform a wide range of continuous actions, such as turning at different angles (left or right 20°, 21,1°, 21,2°) or honking.

Understanding these action spaces is crucial when **choosing RL algorithms** in the future.

To recap:
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/action_space.jpg" alt="Action space recap" width="100%">

## Rewards and Discounting [[rewards]]

In RL, the **reward** is the agent's only feedback. It helps the agent determine whether an action was **good** or ***not**.

The cumulative reward at each time step **t** can be expressed as:

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg" alt="Rewards">
<figcaption>Depiction of how a cumulative reward equals the sum of all rewards in the sequence.
</figcaption>
</figure>

This is equivalent to:

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg" alt="Rewards">
<figcaption>Depiction of a cumulative reward = rt+1 (rt+k+1 = rt+0+1 = rt+1)+ rt+2 (rt+k+1 = rt+1+1 = rt+2) + ...
</figcaption>
</figure>

However, we can't simply **add rewards** like this. Rewards that arrive early (at the game's start) are **more likely to occur** than long-term future rewards.

Imagine your agent as a small mouse, trying to eat as much cheese as possible before being caught by the cat. The mouse can move one tile at a time, just like the cat. The mouse's objective is to eat the maximum amount of cheese (**maximum reward**) before being eaten by the cat (**discount**).

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_3.jpg" alt="Rewards" width="100%">

In this scenario, it's more probable to eat cheese nearby than cheese close to the cat (dangerous territory).

As a result, rewards near the cat, even if larger, are more heavily discounted since we're unsure if we'll reach them.

To incorporate this discounting, we follow these steps:

1. We will be defining a discount rate as **gamma**. This rate value must be **between 0 and 1**. Typically, the values would fall between **0.95 and 0.99**.
- A higher gamma value equals a **higher discount**, meaning that our agent would prioritize **long-term rewards**
- On the other hand, a lower gamma value equals a **lower discount**, meaning that our agent would prioritize **short-term rewards**.

2. Each reward is **discounted** by the value of **gamma** to the exponent of the time step. As the time step increases, the cat would get closer to the mouse, meaning that the **future reward** would be **lower** and would be **less likely** to take place.

Our expected cumulative discounted reward would be:

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg" alt="Rewards" width="100%">
